{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TomaszK98/convolutional_neural_networks/blob/main/Convolutional_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFmVrWzk_ReP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import zipfile\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "from scipy import ndimage\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQkcJ_mLApeR"
      },
      "outputs": [],
      "source": [
        "# Download url of normal and abnormal CT scans.\n",
        "urls = [\n",
        "    \"https://github.com/TomaszK98/convolutional_neural_networks/releases/download/v1.0.0/CT-normal-0.zip\",\n",
        "    \"https://github.com/TomaszK98/convolutional_neural_networks/releases/download/v1.0.0/CT-normal-1.zip\",\n",
        "    \"https://github.com/TomaszK98/convolutional_neural_networks/releases/download/v1.0.0/CT-abnormal-0.zip\",\n",
        "    \"https://github.com/TomaszK98/convolutional_neural_networks/releases/download/v1.0.0/CT-abnormal-1.zip\",\n",
        "    \"https://github.com/TomaszK98/convolutional_neural_networks/releases/download/v1.0.0/CT-normal-test.zip\",\n",
        "    \"https://github.com/TomaszK98/convolutional_neural_networks/releases/download/v1.0.0/CT-abnormal-test.zip\",\n",
        "]\n",
        "file_names = [\n",
        "    \"CT-normal-0.zip\",\n",
        "    \"CT-normal-1.zip\",\n",
        "    \"CT-abnormal-0.zip\",\n",
        "    \"CT-abnormal-1.zip\",\n",
        "    \"CT-normal-test.zip\",\n",
        "    \"CT-abnormal-test.zip\",\n",
        "]\n",
        "\n",
        "for url, file_name in zip(urls, file_names):\n",
        "    filename = os.path.join(os.getcwd(), file_name)\n",
        "    keras.utils.get_file(filename, url)\n",
        "# Make a directory to store the data.\n",
        "os.makedirs(\"CovidCTScans\")\n",
        "\n",
        "# Unzip data in the newly created directory.\n",
        "for file_name in file_names:\n",
        "    with zipfile.ZipFile(file_name, \"r\") as z_fp:\n",
        "        z_fp.extractall(\"./CovidCTScans/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdbTPeAZEYBW"
      },
      "outputs": [],
      "source": [
        "class ProcessScanHandler:\n",
        "    \"\"\"Class responsible for processing CT scans - read images from nifti extension, normalize and resize image\"\"\"\n",
        "    min_volume = -1000\n",
        "    max_volume = 400\n",
        "    desired_depth = 64\n",
        "    desired_height = 128\n",
        "    desired_width = 128\n",
        "\n",
        "    def __init__(self, scans):\n",
        "        self.scans = scans\n",
        "\n",
        "    def run(self):\n",
        "        return np.array([self.process_scan(path) for path in self.scans])\n",
        "\n",
        "    def read_nifti_file(self, filepath):\n",
        "        scan = nib.load(filepath)\n",
        "        return scan.get_fdata()\n",
        "\n",
        "    def normalize(self, volume):\n",
        "        volume[volume < self.min_volume] = self.min_volume\n",
        "        volume[volume > self.max_volume] = self.max_volume\n",
        "        volume = (volume - self.min_volume) / (self.max_volume - self.min_volume)\n",
        "        return volume.astype(\"float32\")\n",
        "\n",
        "    def resize_volume(self, img):\n",
        "        # Get image current size\n",
        "        image_depth = img.shape[-1]\n",
        "        image_width = img.shape[0]\n",
        "        image_height = img.shape[1]\n",
        "        # Calculate depth ratio\n",
        "        depth = image_depth / self.desired_depth\n",
        "        width = image_width / self.desired_width\n",
        "        height = image_height / self.desired_height\n",
        "        depth_ratio = 1 / depth\n",
        "        width_ratio = 1 / width\n",
        "        height_ratio = 1 / height\n",
        "        # Rotate the image\n",
        "        img = ndimage.rotate(img, 90, reshape=False)\n",
        "        # Resize the image across z-axis\n",
        "        return ndimage.zoom(img, (width_ratio, height_ratio, depth_ratio), order=1)\n",
        "\n",
        "    def process_scan(self, path):\n",
        "        volume = self.read_nifti_file(path)\n",
        "        volume = self.normalize(volume)\n",
        "        return self.resize_volume(volume)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkJsEOEGEln0"
      },
      "outputs": [],
      "source": [
        "# Load scan paths from CovidCTScans folder containing all downloaded CT images.\n",
        "normal_scan_paths_1 = [\n",
        "    os.path.join(os.getcwd(), \"CovidCTScans/CT-normal-0\", x)\n",
        "    for x in os.listdir(\"CovidCTScans/CT-normal-0\")\n",
        "]\n",
        "normal_scan_paths_2 = [\n",
        "    os.path.join(os.getcwd(), \"CovidCTScans/CT-normal-1\", x)\n",
        "    for x in os.listdir(\"CovidCTScans/CT-normal-1\")\n",
        "]\n",
        "normal_scan_paths = normal_scan_paths_1 + normal_scan_paths_2\n",
        "\n",
        "abnormal_scan_paths_1 = [\n",
        "    os.path.join(os.getcwd(), \"CovidCTScans/CT-abnormal-0\", x)\n",
        "    for x in os.listdir(\"CovidCTScans/CT-abnormal-0\")\n",
        "]\n",
        "abnormal_scan_paths_2 = [\n",
        "    os.path.join(os.getcwd(), \"CovidCTScans/CT-abnormal-1\", x)\n",
        "    for x in os.listdir(\"CovidCTScans/CT-abnormal-1\")\n",
        "]\n",
        "abnormal_scan_paths = abnormal_scan_paths_1 + abnormal_scan_paths_2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yZL9uiWGEtd"
      },
      "outputs": [],
      "source": [
        "# Process prepared scans using custom ProcessScanHandler(list_of_paths)\n",
        "# Run processing using handler.run() and save it to variables\n",
        "abnormal_scans_handler = ProcessScanHandler(abnormal_scan_paths)\n",
        "normal_scans_handler = ProcessScanHandler(normal_scan_paths)\n",
        "abnormal_scans = abnormal_scans_handler.run()\n",
        "normal_scans = normal_scans_handler.run()\n",
        "\n",
        "# Prepare arrays of labels - assing 1 for abnormal scans and 0 for normal scans\n",
        "abnormal_labels = np.array([1 for _ in range(len(abnormal_scans))])\n",
        "normal_labels = np.array([0 for _ in range(len(normal_scans))])\n",
        "\n",
        "# Split whole dataset to train and validation dataset appropriately 280 and 120 scans\n",
        "x_train = np.concatenate((abnormal_scans[:140], normal_scans[:140]), axis=0)\n",
        "y_train = np.concatenate((abnormal_labels[:140], normal_labels[:140]), axis=0)\n",
        "x_val = np.concatenate((abnormal_scans[140:], normal_scans[140:]), axis=0)\n",
        "y_val = np.concatenate((abnormal_labels[140:], normal_labels[140:]), axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hy-NrSqd5yj"
      },
      "outputs": [],
      "source": [
        "# Prepare additional test dataset - process it and assign labels (keep whole dataset away during model training!)\n",
        "normal_test_scan_paths = [\n",
        "    os.path.join(os.getcwd(), \"CovidCTScans/CT-normal-test\", x)\n",
        "    for x in os.listdir(\"CovidCTScans/CT-normal-test\")\n",
        "]\n",
        "abnormal_test_scan_paths = [\n",
        "    os.path.join(os.getcwd(), \"CovidCTScans/CT-abnormal-test\", x)\n",
        "    for x in os.listdir(\"CovidCTScans/CT-abnormal-test\")\n",
        "]\n",
        "\n",
        "abnormal_test_scans_handler = ProcessScanHandler(abnormal_test_scan_paths)\n",
        "normal_test_scans_handler = ProcessScanHandler(normal_test_scan_paths)\n",
        "abnormal_test_scans = abnormal_test_scans_handler.run()\n",
        "normal_test_scans = normal_test_scans_handler.run()\n",
        "\n",
        "abnormal_test_labels = np.array([1 for _ in range(len(abnormal_test_scans))])\n",
        "normal_test_labels = np.array([0 for _ in range(len(normal_test_scans))])\n",
        "\n",
        "x_test = np.concatenate((abnormal_test_scans, normal_test_scans), axis=0)\n",
        "y_test = np.concatenate((abnormal_test_labels, normal_test_labels), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fAdCz0eQlGD"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def rotate(volume):\n",
        "    \"\"\"Rotate the volume by a few degrees\"\"\"\n",
        "\n",
        "    def rotate_at_random_angles(volume):\n",
        "        # define some rotation angles\n",
        "        list_of_angles = [-20, -10, -5, 5, 10, 20]\n",
        "        # pick angles at random\n",
        "        random_angle = random.choice(list_of_angles)\n",
        "        # rotate volume\n",
        "        volume = ndimage.rotate(volume, random_angle, reshape=False)\n",
        "        volume[volume < 0] = 0\n",
        "        volume[volume > 1] = 1\n",
        "        return volume\n",
        "\n",
        "    augmented_volume = tf.numpy_function(rotate_at_random_angles, [volume], tf.float32)\n",
        "    return augmented_volume\n",
        "\n",
        "\n",
        "def train_dataset_preprocessing(volume, label):\n",
        "    \"\"\"Process training data by rotating and adding a channel.\"\"\"\n",
        "    # Rotate volume\n",
        "    volume = rotate(volume)\n",
        "    volume = tf.expand_dims(volume, axis=3)\n",
        "    return volume, label\n",
        "\n",
        "\n",
        "def validation_and_test_dataset_preprocessing(volume, label):\n",
        "    \"\"\"Process validation data by only adding a channel.\"\"\"\n",
        "    volume = tf.expand_dims(volume, axis=3)\n",
        "    return volume, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dI-0nccQQn-Y"
      },
      "outputs": [],
      "source": [
        "# Prepare data loaders using tensorflow dataset.\n",
        "train_dataset_loader = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "validation_dataset_loader = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "\n",
        "batch_size = 2\n",
        "# Prepare finished datasets, ready to use in model training.\n",
        "train_dataset = (\n",
        "    train_dataset_loader.shuffle(len(x_train))\n",
        "    .map(train_dataset_preprocessing)\n",
        "    .batch(batch_size)\n",
        "    .prefetch(2)\n",
        ")\n",
        "\n",
        "validation_dataset = (\n",
        "    validation_dataset_loader.shuffle(len(x_val))\n",
        "    .map(validation_and_test_dataset_preprocessing)\n",
        "    .batch(batch_size)\n",
        "    .prefetch(2)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlMAWzxSefpU"
      },
      "outputs": [],
      "source": [
        "# Do the same for test dataset.\n",
        "batch_size = 2\n",
        "test_dataset_loader = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "\n",
        "# Prepare finished test dataset - ready to use in evaluate method.\n",
        "test_dataset = test_dataset_loader.map(validation_and_test_dataset_preprocessing).batch(batch_size).prefetch(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJR1javvRBRe"
      },
      "outputs": [],
      "source": [
        "def get_cnn_model(\n",
        "    width: int = 128,\n",
        "    height: int = 128,\n",
        "    depth: int = 64,\n",
        "    filters: int = 32,\n",
        "    filters_step: int = 2,\n",
        "    kernel_size: int = 3,\n",
        "    kernel_size_step: int = 0,\n",
        "    conv_activation: str = \"relu\",\n",
        "    dense_units: int = 4096,\n",
        "    dropout: float = 0.3,\n",
        "    pool_size: int = 2,\n",
        "):\n",
        "\n",
        "    inputs = keras.Input((width, height, depth, 1))\n",
        "\n",
        "    x = layers.Conv3D(\n",
        "        filters=filters, kernel_size=5, activation=conv_activation\n",
        "    )(inputs)\n",
        "    x = layers.MaxPool3D(pool_size=pool_size)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    \n",
        "\n",
        "    x = layers.Conv3D(\n",
        "        filters=filters * filters_step,\n",
        "        kernel_size=kernel_size - kernel_size_step,\n",
        "        activation=conv_activation,\n",
        "    )(x)\n",
        "    x = layers.MaxPool3D(pool_size=pool_size)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    \n",
        "\n",
        "    x = layers.Conv3D(\n",
        "        filters=filters * (filters_step * 2),\n",
        "        kernel_size=kernel_size - kernel_size_step * 2,\n",
        "        activation=conv_activation,\n",
        "    )(x)\n",
        "    x = layers.Conv3D(\n",
        "        filters=filters * (filters_step * 3),\n",
        "        kernel_size=kernel_size - kernel_size_step * 2,\n",
        "        activation=conv_activation,\n",
        "    )(x)\n",
        "    x = layers.MaxPool3D(pool_size=pool_size)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "\n",
        "    x = layers.Conv3D(\n",
        "        filters=filters * (filters_step * 4),\n",
        "        kernel_size=kernel_size - kernel_size_step * 3,\n",
        "        activation=conv_activation,\n",
        "    )(x)\n",
        "    x = layers.Conv3D(\n",
        "        filters=filters * (filters_step * 5),\n",
        "        kernel_size=kernel_size - kernel_size_step * 3,\n",
        "        activation=conv_activation,\n",
        "    )(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "\n",
        "    \n",
        "    x = layers.GlobalAveragePooling3D()(x)\n",
        "    x = layers.Dense(units=dense_units, activation=conv_activation)(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "\n",
        "    outputs = layers.Dense(units=1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    return keras.Model(inputs, outputs, name=\"3dcnn_final\")\n",
        "\n",
        "\n",
        "# Build model.\n",
        "\n",
        "model = get_cnn_model()\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3T_H2xVmRFyo"
      },
      "outputs": [],
      "source": [
        "initial_learning_rate = 0.0001\n",
        "learning_rate = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
        ")\n",
        "metrics = [\n",
        "    tf.keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
        "    tf.keras.metrics.Precision(name=\"precision\"),\n",
        "    tf.keras.metrics.Recall(name=\"recall\"),\n",
        "]\n",
        "model.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "    metrics=metrics,\n",
        ")\n",
        "\n",
        "# Define callbacks.\n",
        "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "    \"3d_image_classification.h5\", save_best_only=True\n",
        ")\n",
        "model_early_stopping_callback = keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=15)\n",
        "\n",
        "# Train the model, doing validation at the end of each epoch\n",
        "epochs = 100\n",
        "model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=epochs,\n",
        "    shuffle=True,\n",
        "    verbose=2,\n",
        "    callbacks=[model_checkpoint_callback, model_early_stopping_callback],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Put7zfjhks_P"
      },
      "outputs": [],
      "source": [
        "# Plot model accuracy and model loss\n",
        "fig_one, ax_one = plt.subplots(1, 2, figsize=(20, 3))\n",
        "ax_one = ax_one.ravel()\n",
        "\n",
        "for i, metric in enumerate([\"accuracy\", \"loss\"]):\n",
        "    ax_one[i].plot(model.history.history[metric])\n",
        "    ax_one[i].plot(model.history.history[\"val_\" + metric])\n",
        "    ax_one[i].set_title(\"Model {}\".format(metric))\n",
        "    ax_one[i].set_xlabel(\"epochs\")\n",
        "    ax_one[i].set_ylabel(metric)\n",
        "    ax_one[i].legend([\"train\", \"val\"])\n",
        "fig_two, ax_two = plt.subplots(1, 2, figsize=(20, 3))\n",
        "ax_two = ax_two.ravel()\n",
        "\n",
        "for i, metric in enumerate([\"precision\", \"recall\"]):\n",
        "    ax_two[i].plot(model.history.history[metric])\n",
        "    ax_two[i].plot(model.history.history[\"val_\" + metric])\n",
        "    ax_two[i].set_title(\"Model {}\".format(metric))\n",
        "    ax_two[i].set_xlabel(\"epochs\")\n",
        "    ax_two[i].set_ylabel(metric)\n",
        "    ax_two[i].legend([\"train\", \"val\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ojMknC4sm6s"
      },
      "outputs": [],
      "source": [
        "model.evaluate(test_dataset, return_dict=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8yO04MiI5yK"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(dataset):\n",
        "    model.load_weights(\"final_weights.h5\")\n",
        "    prediction_labels = model.predict(dataset)\n",
        "    prediction_labels = np.where(prediction_labels > 0.05, 1, 0)\n",
        "    dataset_labels = list(map(lambda x: x[1], dataset))\n",
        "    true_labels = []\n",
        "    for label in dataset_labels:\n",
        "        true_labels.append(label.numpy()[0])\n",
        "        true_labels.append(label.numpy()[1])\n",
        "    ConfusionMatrixDisplay.from_predictions(\n",
        "        true_labels, prediction_labels, display_labels=[\"normalne\", \"anormalne\"]\n",
        "    )\n",
        "    plt.xlabel('Etykieta przewidywana')\n",
        "    plt.ylabel('Etykieta prawdziwa')  \n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWhtqNW-2H1Z"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1H_6BSb2k0pc"
      },
      "outputs": [],
      "source": [
        "# Load best weights. Make prediction based on it.\n",
        "model.load_weights(\"final_weights.h5\")\n",
        "prediction = model.predict(test_dataset)\n",
        "scores = [1 - prediction[0], prediction[0]]\n",
        "average = sum(prediction[48:50]) / len(prediction[48:50])\n",
        "print(np.round(average, 2))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyP/EZOgIPNn51ixdzPoPOEV",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}